{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain: Memory\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "# openai.api_key = \"API_KEY\"\n",
    "\n",
    "# llm_model = \"gpt-3.5-turbo\"\n",
    "llm_model = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature = 0.0, model = llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm = llm,\n",
    "    memory = memory,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# each line will give you an AI response\n",
    "conversation.predict(input = \"Hi, my name is Andrew\")\n",
    "conversation.predict(input=\"What is 1+1?\")\n",
    "conversation.predict(input=\"What is my name?\")\n",
    "\n",
    "# It will remember your name\n",
    "\n",
    "# see saved historical chats\n",
    "print(memory.buffer)\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "# append new chats\n",
    "memory.save_context({\"input\": \"Hi\"}, {\"output\":\"What's up\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Buffer Window Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# k = 1, one last exchange (Human, AI)\n",
    "memory = ConversationBufferWindowMemory(k = 1)\n",
    "conversation = ConversationChain(\n",
    "    llm = llm, \n",
    "    memory = memory,\n",
    "    verbose = False\n",
    ")\n",
    "\n",
    "# each line will give you an AI response\n",
    "conversation.predict(input = \"Hi, my name is Andrew\")\n",
    "conversation.predict(input=\"What is 1+1?\")\n",
    "conversation.predict(input=\"What is my name?\")\n",
    "\n",
    "# It will not remember your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Token Buffer Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the max tokens, this maps more directly to the cost\n",
    "# different llm uses different ways of counting tokens\n",
    "\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm = llm, max_token_limit=50)\n",
    "\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversation Summary Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the LLM to summarize the conversation so far & use as memory\n",
    "# summarized as a System message. e.g. {'history': \"System: Human and AI ...... Human asks ....\\nAI: ....\"}\n",
    "# Other memories: {'history':\"AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!\"}\n",
    "\n",
    "# it tries to keep the explicit storage of the messages up to the # of tokens we have specified as limit (if max_tokens = 100, original msg will kept for <= 100 tokens). \n",
    "# And then anything beyond that, it will use CLM to generate a summary. \n",
    "\n",
    "# e.g. usecase: search on internet, & want to keep facts\n",
    "# others => suitable for chatbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"What would be a good demo to show?\")\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Memory Types\n",
    "- Vector data memory (stored in a vector db & retrieves the most relevant blocks of text)\n",
    "- Entity memories (focus on details about specific entities)\n",
    "\n",
    "- can also use multi-memories at one time. e.g. Conversation + Entity memory to recall individuals\n",
    "\n",
    "- can also store conversation in a conventional database (e.g. key-value store or SQL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
